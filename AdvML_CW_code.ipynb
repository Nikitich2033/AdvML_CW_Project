{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP0162 2023/24 Advanced Machine Learning in Finance Coursework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import calendar\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "import ccxt\n",
    "import mplfinance as mpf\n",
    "\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Initialize the exchange\n",
    "exchange = ccxt.kucoin()\n",
    "\n",
    "# Define the start and end dates for training and testing periods\n",
    "start_date_training = '2024-01-01'\n",
    "end_date_training = '2024-01-15'\n",
    "start_date_test = '2024-01-15'\n",
    "end_date_test = '2024-01-20'\n",
    "\n",
    "# Function to fetch hourly prices for a given period\n",
    "def fetch_hourly_prices_for_period(symbol, start_date, end_date):\n",
    "    since = exchange.parse8601(f'{start_date}T00:00:00Z')\n",
    "    end_timestamp = exchange.parse8601(f'{end_date}T00:00:00Z')\n",
    "    all_ohlcv = []\n",
    "    while since < end_timestamp:\n",
    "        ohlcv = exchange.fetch_ohlcv(symbol, '1h', since=since, limit=1000)  # Fetch a batch of 1000 bars\n",
    "        if len(ohlcv) == 0:  # No more data\n",
    "            break\n",
    "        since = ohlcv[-1][0] + 1  # Prepare the since timestamp for the next query\n",
    "        all_ohlcv.extend(ohlcv)\n",
    "    df = pd.DataFrame(all_ohlcv, columns=['Time', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    df['Time'] = pd.to_datetime(df['Time'], unit='ms')\n",
    "    df.set_index('Time', inplace=True)\n",
    "    return df\n",
    "\n",
    "# Fetch training and testing data for BTC/USDT\n",
    "btc_training = fetch_hourly_prices_for_period('BTC/USDT', start_date_training, end_date_training)\n",
    "btc_test = fetch_hourly_prices_for_period('BTC/USDT', start_date_test, end_date_test)\n",
    "\n",
    "# Fetch training and testing data for MEME/USDT\n",
    "meme_training = fetch_hourly_prices_for_period('MEME/USDT', start_date_training, end_date_training)\n",
    "meme_test = fetch_hourly_prices_for_period('MEME/USDT', start_date_test, end_date_test)\n",
    "\n",
    "# Fetch training and testing data for DOGE/USDT\n",
    "doge_training = fetch_hourly_prices_for_period('DOGE/USDT', start_date_training, end_date_training)\n",
    "doge_test = fetch_hourly_prices_for_period('DOGE/USDT', start_date_test, end_date_test)\n",
    "\n",
    "# Fetch training and testing data for PEPE/USDT\n",
    "pepe_training = fetch_hourly_prices_for_period('PEPE/USDT', start_date_training, end_date_training)\n",
    "pepe_test = fetch_hourly_prices_for_period('PEPE/USDT', start_date_test, end_date_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_training.head(3) # Visualize the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_test.head(3) # Visualize the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pepe_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pepe_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = btc_training.copy()\n",
    "name = 'SimplisticDeepQTrader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Trader:\n",
    "    \n",
    "    def __init__(self, state_size, window_size, trend, skip, batch_size):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.window_size = window_size\n",
    "        self.half_window = window_size // 2 # Round the result down to the nearest whole number.\n",
    "        self.trend = trend\n",
    "        self.skip = skip\n",
    "        self.action_size = 3 # Action 0 -> Hold; Action 1 -> Buy; Action 2 -> Sell.\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen = 1000)\n",
    "        self.inventory = []\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.5\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "        \n",
    "        # Reset the TensorFlow default graph to clear any existing tensors and operations.\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Create an interactive TensorFlow session.\n",
    "        self.sess = tf.InteractiveSession()\n",
    "\n",
    "        # Create a TensorFlow placeholder for the input state of the network.\n",
    "        # The first dimension of the placeholder is \"None\", which means it can take any batch size.\n",
    "        # The second dimension is the size of the state vector.\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.state_size])\n",
    "\n",
    "        # Create a TensorFlow placeholder for the output action of the network.\n",
    "        # The first dimension of the placeholder is \"None\", which means it can take any batch size.\n",
    "        # The second dimension is the size of the action vector.\n",
    "        self.Y = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "\n",
    "        # Create a fully connected layer with 256 units and ReLU activation function.\n",
    "        # The input to this layer is the state vector.\n",
    "        # The output is the hidden layer output.\n",
    "        feed = tf.layers.dense(self.X, 256, activation = tf.nn.relu)\n",
    "\n",
    "        # Create a fully connected layer with size equal to the action vector size.\n",
    "        # The input to this layer is the hidden layer output.\n",
    "        # The output is the output of the network.\n",
    "        self.logits = tf.layers.dense(feed, self.action_size)\n",
    "\n",
    "        # Compute the mean squared error between the output of the network and the target action.\n",
    "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
    "\n",
    "        # Create a gradient descent optimizer with a learning rate of 1e-5.\n",
    "        # Minimize the cost using the optimizer.\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(self.cost)\n",
    "\n",
    "        # Initialize all variables in the TensorFlow graph.\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        # If a random number is less than the exploration rate, select a random action.\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        # Otherwise, given the current state, use the neural network to select the action with the highest probability.\n",
    "        # The action with the highest probability is the one with the highest output value from the logits layer.\n",
    "        return np.argmax(self.sess.run(self.logits, feed_dict = {self.X: state})[0])\n",
    "\n",
    "    \n",
    "    def get_state(self, t):\n",
    "        window_size = self.window_size + 1\n",
    "        d = t - window_size + 1\n",
    "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
    "\n",
    "        res = []\n",
    "        for i in range(window_size - 1):\n",
    "            res.append(block[i + 1] - block[i])\n",
    "        \n",
    "        return np.array([res])\n",
    "\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "\n",
    "        for i in range(l - batch_size, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "\n",
    "        replay_size = len(mini_batch)\n",
    "\n",
    "        # Create empty arrays to hold the state and target data\n",
    "        X = np.empty((replay_size, self.state_size))\n",
    "        Y = np.empty((replay_size, self.action_size))\n",
    "\n",
    "        # Extract the states and next states from the mini batch\n",
    "        states = np.array([a[0][0] for a in mini_batch])\n",
    "        new_states = np.array([a[3][0] for a in mini_batch])\n",
    "\n",
    "        # Obtain Q values for the current and next states\n",
    "        Q = self.sess.run(self.logits, feed_dict={self.X: states})\n",
    "        Q_new = self.sess.run(self.logits, feed_dict={self.X: new_states})\n",
    "\n",
    "        # Loop through the mini batch and update the target Q values\n",
    "        for i in range(len(mini_batch)):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = Q[i]\n",
    "            target[action] = reward\n",
    "\n",
    "            if not done:\n",
    "                target[action] += self.gamma * np.amax(Q_new[i])\n",
    "\n",
    "            X[i] = state\n",
    "            Y[i] = target\n",
    "\n",
    "        # Train the neural network on the state and target data\n",
    "        cost, _ = self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X, self.Y: Y})\n",
    "\n",
    "        # Decay the exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Return the cost of the neural network during training\n",
    "        return cost\n",
    "    \n",
    "    def train(self, iterations, checkpoint, initial_money):\n",
    "        for i in range(iterations):\n",
    "            # Initialize variables for the current episode\n",
    "            total_profit = 0\n",
    "            inventory = []\n",
    "            state = self.get_state(0)\n",
    "            starting_money = initial_money\n",
    "\n",
    "            # Iterate over the time steps in the trend data\n",
    "            for t in range(0, len(self.trend) - 1, self.skip):\n",
    "                # Take an action based on the current state\n",
    "                action = self.act(state)\n",
    "                # Get the next state\n",
    "                next_state = self.get_state(t + 1)\n",
    "\n",
    "                # If the action is \"buy\" and there is enough money to buy, add to inventory\n",
    "                if action == 1 and starting_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n",
    "                    inventory.append(self.trend[t])\n",
    "                    starting_money -= self.trend[t]\n",
    "\n",
    "                # If the action is \"sell\" and there is inventory to sell, sell at current price\n",
    "                elif action == 2 and len(inventory) > 0:\n",
    "                    bought_price = inventory.pop(0)\n",
    "                    total_profit += self.trend[t] - bought_price\n",
    "                    starting_money += self.trend[t]\n",
    "\n",
    "                # Calculate the investment return and add the current step to the replay buffer\n",
    "                invest = ((starting_money - initial_money) / initial_money)\n",
    "                self.memory.append((state, action, invest, next_state, starting_money < initial_money))\n",
    "\n",
    "                # Update the state for the next step and replay the buffer\n",
    "                state = next_state\n",
    "                batch_size = min(self.batch_size, len(self.memory))\n",
    "                cost = self.replay(batch_size)\n",
    "\n",
    "            # Print the progress at the checkpoint interval\n",
    "            if (i+1) % checkpoint == 0:\n",
    "                print(f'Epoch: {i+1} | Total rewards: {total_profit} | Cost: {cost} | Total money {starting_money}')\n",
    "                \n",
    "                \n",
    "    def test(self, initial_money, trend):\n",
    "        self.trend = trend\n",
    "        starting_money = initial_money\n",
    "\n",
    "        states_sell = []\n",
    "        states_buy = []\n",
    "        inventory = []\n",
    "        state = self.get_state(0)\n",
    "        gains_at_each_sale = []  # List to store gains at each sale\n",
    "\n",
    "        for t in range(0, len(self.trend) - 1, self.skip):\n",
    "            action = self.act(state)\n",
    "            next_state = self.get_state(t + 1)\n",
    "\n",
    "            if action == 1 and initial_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n",
    "                inventory.append(self.trend[t])\n",
    "                initial_money -= self.trend[t]\n",
    "                states_buy.append(t)\n",
    "                print(f'Hour {t}: buy 1 unit at price {self.trend[t]}, total balance {initial_money}')\n",
    "\n",
    "            elif action == 2 and len(inventory) > 0:\n",
    "                bought_price = inventory.pop(0)\n",
    "                profit = self.trend[t] - bought_price  # Calculate profit\n",
    "                gains_at_each_sale.append(profit)  # Store the profit\n",
    "                initial_money += self.trend[t]\n",
    "                states_sell.append(t)\n",
    "                print(f'Hour {t}: sell 1 unit at price {self.trend[t]}, profit {profit}, total balance {initial_money}')\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
    "        total_gains = initial_money - starting_money\n",
    "        return states_buy, states_sell, total_gains, invest, gains_at_each_sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_training = btc_training.Close.values.tolist() # Get closing prices.\n",
    "close_test = btc_test.Close.values.tolist() # Get closing prices.\n",
    "\n",
    "initial_money = 100000 # Initial amount of money.\n",
    "window_size = 5 # Lookback window\n",
    "skip = 1 # Overlap regulator.\n",
    "batch_size = 64\n",
    "iterations = 50\n",
    "checkpoint = 5\n",
    "\n",
    "# Initialize and train a Trader agent for BTC/USDT\n",
    "btc_agent = DQN_Trader(state_size=window_size, window_size=window_size, trend=btc_training.Close.values.tolist(), skip=skip, batch_size=batch_size)\n",
    "btc_agent.train(iterations=iterations, checkpoint=checkpoint, initial_money=initial_money)\n",
    "\n",
    "# # Initialize and train a Trader agent for MEME/USDT\n",
    "# meme_agent = DQN_Trader(state_size=window_size, window_size=window_size, trend=meme_training.Close.values.tolist(), skip=skip, batch_size=batch_size)\n",
    "# meme_agent.train(iterations=iterations, checkpoint=checkpoint, initial_money=initial_money)\n",
    "\n",
    "# # Initialize and train a Trader agent for DOGE/USDT\n",
    "# doge_agent = DQN_Trader(state_size=window_size, window_size=window_size, trend=doge_training.Close.values.tolist(), skip=skip, batch_size=batch_size)\n",
    "# doge_agent.train(iterations=iterations, checkpoint=checkpoint, initial_money=initial_money)\n",
    "\n",
    "# # Initialize and train a Trader agent for PEPE/USDT\n",
    "# pepe_agent = DQN_Trader(state_size=window_size, window_size=window_size, trend=pepe_training.Close.values.tolist(), skip=skip, batch_size=batch_size)\n",
    "# pepe_agent.train(iterations=iterations, checkpoint=checkpoint, initial_money=initial_money)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agents with the testing data\n",
    "btc_states_buy, btc_states_sell, btc_total_gains, btc_invest, btc_gains_at_each_sale = btc_agent.test(initial_money=initial_money, trend=btc_test.Close.values.tolist())\n",
    "# meme_states_buy, meme_states_sell, meme_total_gains, meme_invest, meme_gains_at_each_sale = meme_agent.test(initial_money=initial_money, trend=meme_test.Close.values.tolist())\n",
    "# doge_states_buy, doge_states_sell, doge_total_gains, doge_invest, doge_gains_at_each_sale = doge_agent.test(initial_money=initial_money, trend=doge_test.Close.values.tolist())\n",
    "# pepe_states_buy, pepe_states_sell, pepe_total_gains, pepe_invest, pepe_gains_at_each_sale = pepe_agent.test(initial_money=initial_money, trend=pepe_test.Close.values.tolist())\n",
    "\n",
    "# Calculate the cumulative gains for each pair\n",
    "btc_cumulative_gains = np.cumsum(btc_gains_at_each_sale)\n",
    "# meme_cumulative_gains = np.cumsum(meme_gains_at_each_sale)\n",
    "# doge_cumulative_gains = np.cumsum(doge_gains_at_each_sale)\n",
    "# pepe_cumulative_gains = np.cumsum(pepe_gains_at_each_sale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class TradingAgent:\n",
    "    def __init__(self, actions, state_size, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, gamma=0.95, alpha=0.05):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = len(actions)\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.q_table = np.zeros((state_size, self.action_size))\n",
    "        self.actions = actions\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_q_table(self, state, action_index, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
    "        td_delta = td_target - self.q_table[state, action_index]\n",
    "        self.q_table[state, action_index] += self.alpha * td_delta\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, prices):\n",
    "        self.prices = prices['Close'].values\n",
    "        self.n = len(self.prices)\n",
    "        self.current_index = 0\n",
    "        self.cash = 10000\n",
    "        self.holding = False\n",
    "        self.initial_cash = 10000\n",
    "        self.balance_history = []  # To track balance over time\n",
    "        self.pnl_history = []  # To track cumulative PnL\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.holding = False\n",
    "        self.balance_history = [self.initial_cash]\n",
    "        self.pnl_history = [0]\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        return 1 if self.holding else 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if action == 1 and not self.holding:  # Buy\n",
    "            self.holding = True\n",
    "            self.cash -= self.prices[self.current_index]\n",
    "\n",
    "        elif action == 2 and self.holding:  # Sell\n",
    "            self.holding = False\n",
    "            self.cash += self.prices[self.current_index]\n",
    "            reward = self.cash - self.initial_cash\n",
    "\n",
    "        # Update balance and PnL history\n",
    "        self.balance_history.append(self.cash)\n",
    "        self.pnl_history.append(self.cash - self.initial_cash + (self.prices[self.current_index] if self.holding else 0) - self.initial_cash)\n",
    "\n",
    "        self.current_index += 1\n",
    "        if self.current_index >= self.n:\n",
    "            done = True\n",
    "\n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, done\n",
    "\n",
    "def train_agent(episodes=100, btc_prices=btc_training):\n",
    "    env = TradingEnvironment(btc_prices)\n",
    "    agent = TradingAgent(actions=[0, 1, 2], state_size=2)\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return env.balance_history, env.pnl_history\n",
    "\n",
    "# Get balance and cumulative PnL data\n",
    "balance_history, pnl_history = train_agent()\n",
    "\n",
    "# Plotting balance over time\n",
    "plt.figure()\n",
    "plt.plot(balance_history)\n",
    "plt.title('Balance Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Balance')\n",
    "plt.show()\n",
    "\n",
    "# Plotting cumulative PnL\n",
    "plt.figure()\n",
    "plt.plot(pnl_history)\n",
    "plt.title('Cumulative PnL Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative PnL')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming btc_training is a DataFrame containing the fetched BTC hourly prices for training\n",
    "\n",
    "# Define the short-term and long-term window sizes\n",
    "short_window = window_size // 2  # Half of the DQN window size\n",
    "long_window = window_size        # Same as the DQN window size\n",
    "\n",
    "# Calculate the moving averages\n",
    "btc_training['short_mavg'] = btc_training['Close'].rolling(window=short_window, min_periods=1).mean()\n",
    "btc_training['long_mavg'] = btc_training['Close'].rolling(window=long_window, min_periods=1).mean()\n",
    "\n",
    "# Initialize the signal column\n",
    "btc_training['signal'] = np.where(btc_training['short_mavg'] > btc_training['long_mavg'], 1, 0)\n",
    "\n",
    "# Generate trading orders\n",
    "btc_training['positions'] = btc_training['signal'].diff()\n",
    "\n",
    "# Initialize portfolio with the same budget as DQN_Trader: 100,000 units\n",
    "initial_capital = 100000\n",
    "capital = initial_capital\n",
    "btc_held = 0\n",
    "cum_pnl = []  # List to track cumulative PnL\n",
    "\n",
    "for i, (price, position) in enumerate(zip(btc_training['Close'], btc_training['positions'])):\n",
    "    if position == 1:  # Buy\n",
    "        btc_held += 1\n",
    "        capital -= price\n",
    "    elif position == -1 and btc_held > 0:  # Sell\n",
    "        btc_held -= 1\n",
    "        capital += price\n",
    "\n",
    "    # Calculate and store cumulative PnL\n",
    "    current_pnl = capital + btc_held * price - initial_capital\n",
    "    cum_pnl.append(current_pnl)\n",
    "\n",
    "    if i % trade_checkpoint == 0:\n",
    "        print(f\"Step {i}: Balance {capital}, Cumulative PnL {current_pnl}\")\n",
    "\n",
    "# Final Portfolio Value and Total Profit\n",
    "final_value = capital + btc_held * btc_training.iloc[-1]['Close']\n",
    "total_profit = final_value - initial_capital\n",
    "print(f\"Final portfolio value: {final_value}\")\n",
    "print(f\"Total profit: {total_profit}\")\n",
    "\n",
    "# Plot Cumulative PnL over time\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(cum_pnl, label='Cumulative PnL')\n",
    "plt.title('Cumulative PnL over Time Using Double MA Crossover')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative PnL')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
